# -*- coding: utf-8 -*-
"""TwitterSentiment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YHbxVIpKK9m01KW7GRUb8ZnmR6Q0_7Ik

# Data Processing
"""

import numpy as np
import pandas as pd
pd.options.mode.chained_assignment = None
from wordcloud import WordCloud
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
import string
import nltk
import re
#!pip install emoji
import emoji
nltk.download('punkt_tab')
from nltk import word_tokenize
from nltk.corpus import stopwords
nltk.download('stopwords')

from google.colab import files
uploaded = files.upload()

validation = pd.read_csv('twitter_validation.csv', header = None)
training = pd.read_csv('twitter_training.csv', header= None)

training.head()

# Datasets have no headers, so set headers

training.columns = ['number', 'topic', 'sentiment', 'text']
validation.columns = ['number', 'topic', 'sentiment', 'text']

training_data = training
validation_data = validation

"""## Text Cleaning"""

# Convert to lowercase
training_data["new"] = training_data.text.str.lower()
validation_data["new"] = validation_data.text.str.lower()

# Convert all to string
training_data["new"] = [str(data) for data in training_data.new]
validation_data["new"] = [str(data) for data in validation_data.new]

# First convert ’ apostrophes to ' because translate class does not recognize curly (’)
training_data["new"] = [data.replace("’","'") for data in training_data.new]
validation_data["new"] = [data.replace("’","'") for data in validation_data.new]

# Remove punctuation
training_data["new"] = [data.translate(str.maketrans('', '', string.punctuation)) for data in training_data.new]
validation_data["new"] = [data.translate(str.maketrans('', '', string.punctuation)) for data in validation_data.new]

# Remove extra spaces
training_data["new"] = [re.sub(r'\s+', ' ', data).strip() for data in training_data.new]
validation_data["new"] = [re.sub(r'\s+', ' ', data).strip() for data in validation_data.new]

# Remove stopwords (my, is, the, etc)
stop = set(stopwords.words('english'))
training_data["new"] = [" ".join([word for word in data.split() if word not in stop]) for data in training_data.new]
validation_data["new"] = [" ".join([word for word in data.split() if word not in stop]) for data in validation_data.new]

# Converting emojis to text
training_data["new"] = [emoji.demojize(data) for data in training_data.new]
validation_data["new"] = [emoji.demojize(data) for data in validation_data.new]

validation_data.head()

"""## Observing Data"""

word_cloud_text = ''.join(training_data[training_data["sentiment"]=="Positive"].new)
wordcloud = WordCloud(max_font_size=80, max_words=100, background_color="white", scale=10, width=800, height=800).generate(word_cloud_text)
plt.figure(figsize=(6,6))
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis("off")
plt.title("Most Common Words for Positive Sentiment")
plt.show()

from collections import Counter
import matplotlib.pyplot as plt

word_cloud_text = ' '.join(training_data[training_data["sentiment"] == "Positive"].new)
word_counts = Counter(word_cloud_text.split())
common_words = word_counts.most_common(20)
words, frequencies = zip(*common_words)

plt.figure(figsize=(10, 4))
plt.bar(words, frequencies, color="skyblue")
plt.xticks(rotation=45, ha="right")
plt.title("Most Common Words for Positive Sentiment")
plt.xlabel("Words")
plt.ylabel("Frequency")
plt.tight_layout()
plt.show()

"""# Logistic Regression Model"""

bow_counts = CountVectorizer(tokenizer=word_tokenize, ngram_range=(1, 1))

train, test = train_test_split(training_data, test_size = 0.2, random_state = 0)

X_train_bow = bow_counts.fit_transform(train.new)
X_test_bow = bow_counts.transform(test.new)

Y_train_bow = train['sentiment']
Y_test_bow = test['sentiment']

model1 = LogisticRegression(C=1, solver="liblinear",max_iter=200)
model1.fit(X_train_bow, Y_train_bow)

test_pred = model1.predict(X_test_bow)
print("Accuracy: ", accuracy_score(Y_test_bow, test_pred) * 100)

#Validation data
X_val_bow = bow_counts.transform(validation_data.new)
y_val_bow = validation_data['sentiment']

Val_res = model1.predict(X_val_bow)
print("Accuracy: ", accuracy_score(y_val_bow, Val_res) * 100)
